{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://thespectrumofriemannium.files.wordpress.com/2012/02/it.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain the general principles of Information Theory\n",
    "- Define Entropy in the context of Information Theory\n",
    "- Explain how Entropy can be used in Statistics / Machine Learning\n",
    "- Calculate the Entropy of common statistical events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information Theory (IT)\n",
    "------\n",
    "<center><img src=\"https://www.researchgate.net/profile/Fareed_Al-Hindawi/publication/313928362/figure/fig2/AS:493421879140353@1494652351277/Figure-no-2-Shannon-Weaver-Information-Theory-1949-This-diagram-refers-to-the.png\" width=\"90%\"/></center>\n",
    "The goal of IT is to define the fundamental limits on signal processing and communication, such as data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information Theory Greatest Hits\n",
    "------\n",
    "\n",
    "- Modern computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Internet\n",
    "- Telecommunications systems, including mobile phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Computational linguists modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Understanding of black holes\n",
    "- Voyager missions to deep space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do we care about IT?\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://bricaud.github.io/personal-blog/images/entropy/splitdiagram.png\" width=\"75%\"/></center>\n",
    "\n",
    "Information Theory is how decision tree actually decide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://stanstudio.com/wp-content/uploads/2012/02/Claude_Shannon_Juggling.jpg\" width=\"40%\"/></center>\n",
    "\n",
    "Claude E. Shannon proposed it in __1948__ in his landmark paper entitled \"A Mathematical Theory of Communication\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"Entropy\" is the key concept in information theory\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://imagecache5d.allposters.com/watermarker/62-6264-ZSG5100Z.jpg?ch=671&cw=894&type=cns\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Physics\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://d2jmvrsizmvf4x.cloudfront.net/rvkaVrvITeYKGO3EMmeG_entropy.jpg\" width=\"83%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Entropy in Thermodynamics\n",
    "-----\n",
    "\n",
    "The second law of thermodynamics states that the total entropy of an isolated system can only increase over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br>\n",
    "<center><img src=\"http://s2.quickmeme.com/img/cd/cd9ac5d71167288007fe7d9b45dc8faa7229529028a54797cba49b55e0700963.jpg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Information Theory\n",
    "-----\n",
    "\n",
    "Claude Shannon defined the fundamental units of __information__, the smallest possible chunk that cannot be divided any further.\n",
    "\n",
    "He coined the \"bits\". Bit is short for binary digit: 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Groups of bits which can be used to encode __any__ message. \n",
    "\n",
    "All messages can be digitized, hence the digital revolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shannon entropy is the quantity $H$\n",
    "------\n",
    "\n",
    "$$H = -\\sum p(x)log p(x)$$\n",
    "\n",
    "Sum up the probabilities of the all possible symbols (x) that might turn up in a message, weighted by the number of bits needed to represent the value of that symbol (x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logarithms review\n",
    "-----\n",
    "\n",
    "<br>\n",
    "<center><img src=\"http://science.larouchepac.com/gauss/ceres/InterimII/Arithmetic/Primes/Log_Exp_inverts.jpg\" width=\"400\"/></center>\n",
    "\n",
    "A logarithm is an inverse exponential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "b<sup>x</sup> = y is equivalent to saying x = log<sub>b</sub>y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Properties of exponentials and logarithms\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"https://people.richland.edu/james/lecture/m116/logs/log2.gif\" width=\"300\"/></center>\n",
    "\n",
    "Exponential functions grow at a distressingly fast rate, as anyone who has ever tried to pay off a credit card balance understands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, logarithms, inverse exponential functions, grow very slowly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logarithms arise in any process where things are repeatedly halved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is binary search? Why is it fast? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given a sorted array, we start in the middle n/2 and look for our item. If we don't find it we take either the upper or lower half and repeat until we find it.\n",
    "\n",
    "Binary search is a good example of an O(log<sub>2</sub>n) algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is the maximum of binary searches will it find an item in a million?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "palette = \"Dark2\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.93\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from math import ceil, log\n",
    "\n",
    "print(f\"{log(1_000_000, 2):.4}\")\n",
    "print(ceil(log(1_000_000, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2># of Bits to encoding options</h2>\n",
    "\n",
    "1 bit, there are 2 options: {0, 1}\n",
    "\n",
    "2 bits, there are 4 options: {00, 01, 10, 11}\n",
    "\n",
    "3 bits, there are 8 options: {000, 001, 010, 011, 100, 101, 110, 111}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Bits & Logarithms\n",
    "--------\n",
    "\n",
    "How many bits do we need to represent any one of n different possibilities?\n",
    "\n",
    "That could one of n items or the integers from 1 to n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The key observation is that there must be at least $n$ different bit patterns of length $w$. \n",
    "\n",
    "Since the number of different bit patterns doubles as you add each bit, we need at least $w$ bits where 2<sup>w</sup> = n.\n",
    "\n",
    "We need w = log<sub>2</sub>n bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def print_items_n_bits(nums):\n",
    "    \"Give a list of numbers, print the number bits needed to encode.\"\n",
    "    print(f\"{'# of items':>10} | {'# of bits to encode':>9}\")\n",
    "    for n in nums:\n",
    "        print(f\"{n:>10,} {ceil(log(n, 2)):>9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items | # of bits to encode\n",
      "         1         0\n",
      "         2         1\n",
      "         3         2\n",
      "         4         2\n",
      "         5         3\n",
      "         6         3\n",
      "         7         3\n",
      "         8         3\n",
      "         9         4\n",
      "        10         4\n"
     ]
    }
   ],
   "source": [
    "nums = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print_items_n_bits(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items | # of bits to encode\n",
      "        10         4\n",
      "     1,000        10\n",
      "    10,000        14\n",
      "   100,000        17\n",
      " 1,000,000        20\n",
      "10,000,000        24\n"
     ]
    }
   ],
   "source": [
    "nums = [10, 1_000, 10_000, 100_000, 1_000_000, 10_000_000]\n",
    "print_items_n_bits(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy Formula\n",
    "-------\n",
    "\n",
    "$$H = -\\sum p(x)log_2 p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each symbol in a message, find the probability then multiple it by the log of it. \n",
    "\n",
    "Sum up all of those and take the negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The base-2 measure of entropy has sometimes been called the \"Shannon Entropy\" in Claude Shannon's honor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Base-2 logarithms \n",
    "------\n",
    "\n",
    "Base-2 logarithms create units called __bits__ (or shannons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For now, we are only going to use base-2 and discrete items. \n",
    "\n",
    "Information Theory cant extend to other bases and continuous levels of measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Natural logarithm, with base e, then our units would be __nats__. \n",
    "\n",
    "One nat is the amount of information gained by observing an event of probability 1/e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Statistics\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/im_so_random.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is the measure of the uncertainty in a random variable.\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, the entropy of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The entropy of variable X is defined as:  \n",
    "\n",
    "H(X) = -Œ£ p(x) ‚Ä¢ log<sub>2</sub>p(x)\n",
    "\n",
    "p(x) = Pr{X = x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy of a single coin flip\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://68.media.tumblr.com/bb49b60c0e17387aef2d3da24f0ef40a/tumblr_n30n5iMJgT1sa11jco1_500.gif\" width=\"60%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is the statistical term we use to model a single toss of a coin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Bernoulli trial__: There is a single outcome of two possible for each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Can we use a Bernoulli process, a series of independent Bernoulli trials, model unfair coins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes. We can model the probabilities of coming up heads (not heads /tails) as a number between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we have a fair coin, the probability occurring p(heads) and p(tails) are each 50%.\n",
    "\n",
    "H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "H = -(.5*log(.5, 2) + .5*log(.5, 2))\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we have flipped a coin, we have gained one bit of information or reduced our uncertainty by one bit.\n",
    "\n",
    "This makes intuitive sense: 0 = Heads, 1 = Tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy of a single \"weighted\" coin flip\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"https://izbicki.me/img/uploads/2011/11/coins-all.jpg\" height=\"500\"/></center>\n",
    "\n",
    "p(H) = .2  \n",
    "P(¬¨H) = 1-.2 = .8\n",
    "\n",
    "What is H? \n",
    "\n",
    "H = -Œ£(pi ‚Ä¢ log2(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "from numpy import log2\n",
    "\n",
    "H = -(.2*log2(.2) + .8*log2(.8))\n",
    "print(f\"{H:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "---- \n",
    "\n",
    "Is there more or less entropy in a the biased coin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Less entropy__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p_heads = .5\n",
    "H = -(p_heads*log2(p_heads) + (1-p_heads)*log2(1-p_heads))\n",
    "print(f\"{H:.2}\") # Way less entropy, aka far more predicatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p_heads = 0\n",
    "\n",
    "# What will this be?\n",
    "# H = -(p_heads*log2(p_heads) + (1-p_heads)*log2(1-p_heads))\n",
    "# print(f\"{H:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Binary entropy function\n",
    "------\n",
    "\n",
    "The special case of information entropy for a random variable with two outcomes:\n",
    "<br><br>\n",
    "<center><img src=\"images/bin_ent.svg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bernoulli random variable entropy as a function of probability of success.\n",
    "------\n",
    "\n",
    "<center><img src=\"images/curve.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/curve.png\" width=\"500\"/></center>\n",
    "\n",
    "What is the maximum entropy value? At which probability of success will that occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The maximum entropy is H=1 and will happen when P(Success) = .5, aka discrete uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Which has less entropy a flip of fair coin or the roll of a fair dice? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fair Coin Flip\n",
    "\n",
    "Identifying the outcome of a fair coin flip (2 equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (6 equally likely outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/events.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is the entropy of a 2 (fair) coin flips?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since they are independent,   \n",
    "H(Coin 1) + H(Coin 2) = H(2 Coins)  \n",
    "1 + 1 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "H = -(.5*log2(.5) + \n",
    "      .5*log2(.5) +\n",
    "      .5*log2(.5) +\n",
    "      .5*log2(.5))\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$H$ is a measure of the information in a message.\n",
    "------\n",
    "\n",
    "$$H = -\\sum p(x)log_2 p(x)$$\n",
    "\n",
    "Sum up the probabilities of the all possible symbols (x) that might turn up in a message, weighted by the number of bits needed to represent the value of that symbol (x).\n",
    "\n",
    "What is the Shannon entropy of different messages that are 5 letter long _ _ _ _ _?\n",
    "\n",
    "1. `j-u-i-c-y`\n",
    "1. `a-a-a-a-a`\n",
    "1. `h-e-l-l-o`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Shannon entropy of  `j-u-i-c-y`?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of each symbol:  \n",
    "p(\"j\") = 1/5   \n",
    "p(\"u\") = 1/5   \n",
    "p(\"i\") = 1/5  \n",
    "p(\"c\") = 1/5    \n",
    "p(\"y\") = 1/5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__The PMF__: p(\"j\") = p(\"u\") =  p(\"i\") = p(\"c\") = p(\"y\") = 1/5   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.32\n"
     ]
    }
   ],
   "source": [
    "H = -((1/5)*log2(1/5) + \n",
    "      (1/5)*log2(1/5) +\n",
    "      (1/5)*log2(1/5) +\n",
    "      (1/5)*log2(1/5) +\n",
    "      (1/5)*log2(1/5))\n",
    "print(f\"{H:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Shannon entropy of `a-a-a-a-a`?\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "H = -((5/5)*log2(5/5))\n",
    "print(f\"{H:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Shannon entropy of  `h-e-l-l-o`?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of each symbol:  \n",
    "p(\"h\") = 1/5   \n",
    "p(\"e\") = 1/5   \n",
    "p(\"l\") = 1/5  \n",
    "p(\"l\") = 1/5    \n",
    "p(\"o\") =  1/5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The PMF:  \n",
    "----\n",
    "p(\"h\") = p(\"e\") =  p(\"0\") = 1/5   \n",
    "p(\"l\") = 2/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92\n"
     ]
    }
   ],
   "source": [
    "H = -((1/5)*log2(1/5) + \n",
    "      (1/5)*log2(1/5) +\n",
    "      (1/5)*log2(1/5) +\n",
    "      (2/5)*log2(2/5))\n",
    "print(f\"{H:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "If H = 1.92, how many bits do you need per symbol to encode \"hello\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2 bits.\n",
    "\n",
    "The entropy is 1.92 but bits are discrete / integers so we take the ceiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We could use 3 bits (or more) but that would inefficient.\n",
    "\n",
    "Entropy gives us the minimum of bit to encode a message without loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shannon entropy\n",
    "-----\n",
    "\n",
    "The average minimum number of bits needed to encode a symbol from a string of symbols, based on the probability of each symbol appearing in that string.\n",
    "\n",
    "\n",
    "1. `j-u-i-c-y` H = 2.32, thus need 3 bits to transmit\n",
    "1. `a-a-a-a-a` H = 0, thus need 0 bits to transmit\n",
    "1. `h-e-l-l-o` H = 1.92, thus need 2 bits to transmit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "True or False:\n",
    "\n",
    "Entropy is always non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__True__: Entropy is always non-negative. \n",
    "\n",
    "It can be zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in other words\n",
    "------\n",
    "\n",
    "Entropy is the minimum descriptive complexity of a random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/self.svg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, I(x) is the self-information, which is the entropy contribution of an individual message, and ùîº<sub>X</sub> is the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A property of entropy is that it is maximized when all the messages in the message space are equiprobable p(x) = 1/n; i.e., most unpredictable, in which case H(X) = log n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "When does a system have zero entropy? What would call in statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is a single state, aka not a Random Variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in yet other words\n",
    "------\n",
    "\n",
    "Entropy is the lower bound on the average number of yes/no questions to guess the state of a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember: Binary search (yes/no) to guess where the needle is in a sorted haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy (one last time)\n",
    "-----\n",
    "\n",
    "Entropy is sometimes called a measure of surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Which discrete distribution has the maximum entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Discrete uniform distribution has the maximum entropy\n",
    "--------\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1f/Uniform_discrete_pmf_svg.svg\" width=\"75%\"/></center>\n",
    "\n",
    "Knowing the distribution gives us no prior information.\n",
    "\n",
    "It is the most surprising distribution to sample from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which has more entropy: ice or water?\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/glasses.png\" width=\"55%\"/></center>\n",
    "<center>Think, Pair, Share</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Liquid water has more entropy than ice, even crushed ice.\n",
    "-----\n",
    "\n",
    "The jumble of ice chips may look more disordered in comparison to the glass of water which looks uniform and homogeneous. \n",
    "\n",
    "But the ice chips place limits on the number of ways the molecules can be arranged. \n",
    "\n",
    "The water molecules in the glass of water can be arranged in many more ways; therefore the glass of water has greater entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Source: https://www.quora.com/Which-thing-has-more-entropy-liquid-water-or-frozen\n",
    "\n",
    "https://socratic.org/questions/which-has-more-entropy-ice-liquid-water-or-water-vapor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Continuous distributions tend to have more entropy than distributions\n",
    "----\n",
    "\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/water_delivery.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Information theory is concerned with representing data in a compact fashion.\n",
    "- Entropy is the measure of the uncertainty in a random variable.\n",
    "- The entropy of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known.\n",
    "- Entropy is calculated as: H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))\n",
    "- That single metric allows us to compare across domains and random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In other words, Shannon Entropy is the ‚Ä¶\n",
    "------\n",
    "\n",
    "Absolute limit on the best possible lossless encoding or compression of any communication assuming that the communication may be represented as a sequence of i.i.d. random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The entropy of a source that emits a sequence of N symbols that are independent and identically distributed (iid) is N¬∑H bits (per message of N symbols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"http://www.science4all.org/wp-content/uploads/2013/03/Noisy-Communication2.png\" width=\"60%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is time's arrow\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Entropy Symbol\n",
    "-------\n",
    "<br>\n",
    "<center><img src=\"images/e_f.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "‚Ñç is very ugly Unicode so I'll just use H."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is one of many measures of uncertainty!\n",
    "\n",
    "[Learn more here](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/active/slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let X be a discrete random variable with alphabet ùìß and probability mass function p(x)\n",
    "\n",
    "p(x) = Pr{X = x}, x ‚àà ùìß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The alphabet ùìß is all possible outcomes (I know that is yet another \"x\" but we need it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is the expected ‚Äúcompressibility‚Äù of a single bit under the best encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Using IT to calculate how many tweets could there be?](https://what-if.xkcd.com/34/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Differential entropy\n",
    "-----\n",
    "\n",
    "Differential entropy (also referred to as continuous entropy) is a concept in information theory that began as an attempt by Shannon to extend the idea of (Shannon) entropy, a measure of average surprisal of a random variable, to continuous probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Which discrete distribution has the minimum entropy (which is zero)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Dirac delta function__, aka an impulse function.\n",
    "\n",
    "All its probability mass is in a single state. The distribution has no uncertainty, zero entropy.\n",
    "\n",
    "Knowing the distribution gives us all the prior information we could use. It is the least surprising distribution to sample from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "What is the Shannon entropy of the English alphabet, <br>assuming each character is equally likely?\n",
    "-----\n",
    "\n",
    "English has 26 letters (a-z). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7004\n"
     ]
    }
   ],
   "source": [
    "n = 26\n",
    "H = -((1/n)*log2(1/n)*n)\n",
    "print(f\"{H:.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7004\n"
     ]
    }
   ],
   "source": [
    "print(f\"{log2(n):.5}\") # Or reduce to more simple form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "https://www.quora.com/What-is-the-27th-letter-of-the-English-alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
