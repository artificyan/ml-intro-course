{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Ensemble Learning, The Wisdom of the Crowds</h2></center>\n",
    "\n",
    "<center><img src=\"https://www.ibmbigdatahub.com/sites/default/files/styles/open_graph_image/public/blog_iotcrowd_thumbnail.jpg?itok=XQEqPMWi\" width=\"90%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define ensembling in your own words.\n",
    "- Explain why ensembling is a useful ML technique.\n",
    "- Define and give examples of 3 most common ensembling methods:\n",
    "    1. Stacking\n",
    "    2. Bagging\n",
    "    3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ensemble Methods, aka Ensembling\n",
    "------\n",
    "\n",
    "Combine multiple ML models to obtain better predictive performance than any of single models could do alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Techniques for combining several weak learners to produce a single strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How the weak  become strong together\n",
    "-------\n",
    "\n",
    "<center><img src=\"http://jasonleaster.github.io/images/img_for_2015_12_13/stump.png\" width=\"55%\"/></center>\n",
    "\n",
    "Suppose we have a decision stump classifier with 70% accuracy, but Bayes Error is 0%.   \n",
    "Is this good enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Student Activity: Think, Pair, & Share\n",
    "------\n",
    "\n",
    "Suppose we have 3 completely independent decision stump classifiers each with an accuracy of 70%.\n",
    "\n",
    "If we take a majority vote, what is the accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\binom{3}{2}(.7)^2(.3)^1 + \\binom{3}{3}(.7)^3(.3)^0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import operator as op\n",
    "from functools import reduce\n",
    "\n",
    "def n_choose_k(n, k):\n",
    "    k = min(k, n-k)\n",
    "    numerator = reduce(op.mul, range(n, n-k, -1), 1)\n",
    "    denominator = reduce(op.mul, range(1, k+1), 1)\n",
    "    return numerator//denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.4%\n"
     ]
    }
   ],
   "source": [
    "majority_accuracy = ((n_choose_k(3, 2)*(.7**2)*(.3**1)) + \n",
    "                     (n_choose_k(3, 3)*(.7**3)*(.3**0)))\n",
    "print(f\"{majority_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "majority_accuracy = ((binom(3, 2)*(.7**2)*(.3**1)) + \n",
    "                     (binom(3, 3)*(.7**3)*(.3**0)))\n",
    "print(f\"{majority_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we now have 5 completely independent decision stump classifiers each with an accuracy of 70%.\n",
    "\n",
    "What is the majority vote accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\binom{5}{3}(.7)^3(.3)^2 + \\binom{5}{4}(.7)^4(.3)^1 + \\binom{5}{5}(.7)^5(.3)^0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.7%\n"
     ]
    }
   ],
   "source": [
    "majority_accuracy = ((n_choose_k(5, 3)*(.7**3)*(.3**2)) + \n",
    "                     (n_choose_k(5, 4)*(.7**4)*(.3**1)) + \n",
    "                     (n_choose_k(5, 5)*(.7**5)*(.3**0)))\n",
    "print(f\"{majority_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we had 101 such classifiers, we would have 99.9% majority vote accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bagging, aka Bootstrap Aggregating\n",
    "----\n",
    "\n",
    "<center><img src=\"https://www.oreilly.com/library/view/python-machine-learning/9781783555130/graphics/3547_07_06.jpg\" width=\"50%\"/></center>\n",
    "\n",
    "Fit multiple models in parallel and independently.\n",
    "\n",
    "Each model gets a vote on the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bootstrap Statistical Procedure\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/10/bootstrap-sample.png\" width=\"75%\"/></center>\n",
    "\n",
    "Create many random subsamples with replacement. Compute the statistic of each subsample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bootstrap Sampling Steps\n",
    "-----\n",
    "\n",
    "1. Start with your dataset of size $n$\n",
    "1. Sample from your dataset with replacement to create 1 bootstrap sample of size $n$ which means many of the observations will be repeated\n",
    "1. Repeat $B$ times\n",
    "1. Each bootstrap sample can then be used as a separate dataset for model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bootstraping\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/image20-768x289.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bagging\n",
    "----\n",
    "\n",
    "<center><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-08-13-11-49-768x580.png\" width=\"55%\"/></center>\n",
    "\n",
    "You can bag with any collection of algorithms, giving them each a vote in the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Although usually applied to tree-based algorithms, it can be used with any type of algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For regression problems (predicting a continuous value), we average the values given by all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For classification problems (predicting a categorical value), we choose the label with the most votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators = [('lr', lr_clf), \n",
    "                                            ('dt', dt_clf), \n",
    "                                            ('svc',svm_clf)],\n",
    "                              voting = 'hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test) # üçæ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Types of Voting\n",
    "------\n",
    "\n",
    "__Hard voting__: A model is selected from an ensemble to make the final prediction by a simple majority vote for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Soft Voting__: Can only be done when all your classifiers can calculate probabilities for the outcomes. \n",
    "\n",
    "Soft voting averages out the probabilities calculated by individual algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do Bagging?\n",
    "------\n",
    "\n",
    "- Increases evaluation metric performance.\n",
    "- Less likely to overfitting since permutations of datasets are fit.\n",
    "- Improves stability of estimates. If ML people ever made error bars, they would be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Stacking (not just for Transformers) \n",
    "-----\n",
    "\n",
    "An ensemble learning technique that uses predictions from previous models to build a new model. \n",
    "\n",
    "Two variations:\n",
    "\n",
    "1. Pipeline\n",
    "1. Metalearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Stacking with a Metalearner\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://hsto.org/getpro/habr/post_images/c28/f6a/e29/c28f6ae298041c65eba7a97d3fbcce8e.png\" width=\"75%\"/></center>\n",
    "\n",
    "Metalearner learns the optimal combination of the base learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another common example of Stacking\n",
    "-----\n",
    "\n",
    "1. First, clustering (or topic modeling). \n",
    "2. Then fit a separate classifier for each cluster. Different clusters may have different feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Challenge Question\n",
    "-------\n",
    "\n",
    "If a data point is incorrectly predicted by the several of the models for a systematical reason, will combining the predictions provide better results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No - We need a method to improve the errors across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Boosting\n",
    "----\n",
    "\n",
    "<center><img src=\"https://littleml.files.wordpress.com/2017/03/boosted-trees-process.png\" width=\"100%\"/></center>\n",
    "\n",
    "A sequential process, where each subsequent model attempts to correct the errors of the previous models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Boosting algorithms\n",
    "-----\n",
    "\n",
    "- XGBoost (Currently, the most popular)\n",
    "- AdaBoost\n",
    "- Gradient Boosting Machine (GBM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> ML 2 will cover boosting</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When should you choose Stacking?\n",
    "------\n",
    "\n",
    "It is generally a good idea to pipe the outputs of one model into the inputs of another model.\n",
    "\n",
    "However, creating a meta-learner that is able to choose among heterogeneous models is often too complex for the gain in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When should you choose Bagging?\n",
    "------\n",
    "\n",
    "If you have time and enough data, bagging is a good choice because it only improves model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When should you choose Boosting?\n",
    "------\n",
    "\n",
    "Boosting is good idea if highest level model performance is required. \n",
    "\n",
    "However, it is more complex than Bagging (harder to implement and harder to debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Will Bagging increase or decrease Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Never increase Bias. Most of the time decrease Bias.\n",
    "\n",
    "The final prediction error is often lower than any individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Will Bagging increase or decrease Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Never increase Variance. Most of the time decrease Variance.\n",
    "\n",
    "By combining predictions, they will not overfit to specific attributes of certain training sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "How does Bagging partition the subsetted data?  \n",
    "How does Boosting partition the subsetted data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bagging does random partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Boosting samples data with errors at a higher preference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Summary\n",
    "------\n",
    "\n",
    "- Ensembles are collections of model that will perform better than any individual model.\n",
    "- Stacking is chaining the output of one model as the input of another model.\n",
    "- Boosting is where subsequent models learn to correct the errors of earlier models.\n",
    "- Bagging is repeatedly sampling the training data and fitting a model to each resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
